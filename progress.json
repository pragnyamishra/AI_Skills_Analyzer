{
  "started_date": "2026-01-29",
  "total_challenges": 7,
  "completed_challenges": 0,
  "challenges": [
    {
      "date": "2026-01-29",
      "challenge": "**Challenge Title:** Data Processing with Databricks and Pig\n\n**What You'll Build/Create:** A Databricks notebook that uses Apache Pig to process a sample dataset and generate insights.\n\n**Skills Practiced:** \n- Databricks\n- Apache Pig\n- Data processing and analysis\n\n**Step-by-Step Instructions:**\n\n1. **Sign up for a Databricks account**: Go to the Databricks website and sign up for a free trial account. Follow the instructions to set up your account and create a new workspace.\n2. **Create a new cluster**: In your Databricks workspace, create a new cluster with the default settings. This will be used to run your Apache Pig jobs.\n3. **Upload a sample dataset**: Upload a sample dataset (e.g., a CSV file) to your Databricks workspace. You can use a public dataset such as the \"flights\" dataset from the Databricks dataset library.\n4. **Create a new Databricks notebook**: Create a new Databricks notebook and select \"Python\" as the language.\n5. **Install Apache Pig**: In your notebook, install Apache Pig using the following command: `%pyspark`\n   Then, import the necessary libraries: `from pyspark.sql import SparkSession`\n6. **Load the dataset into a Pig relation**: Use the following command to load the dataset into a Pig relation: `flights = spark.read.csv(\"path_to_your_dataset\", header=True, inferSchema=True)`\n7. **Process the data using Pig**: Use Pig Latin to process the data. For example, you can use the `FILTER` command to filter out rows with missing values: `filtered_flights = flights.filter(flights[\"column_name\"].isNotNull())`\n8. **Generate insights**: Use Pig to generate insights from the processed data. For example, you can use the `GROUP BY` command to calculate the average value of a column: `average_values = filtered_flights.groupBy(\"column_name\").avg(\"another_column_name\")`\n9. **Display the results**: Display the results of your analysis using the `show()` method: `average_values.show()`\n\n**Success Criteria:**\n- You have successfully loaded the dataset into a Pig relation.\n- You have processed the data using Pig and generated insights.\n- You have displayed the results of your analysis.\n\n**How This Helps Your Data Engineer Job Search:**\nThis challenge demonstrates your ability to work with Databricks and Apache Pig to process and analyze large datasets. By completing this challenge, you can showcase your skills in:\n- Data processing and analysis\n- Using Databricks and Apache Pig\n- Generating insights from large datasets\nYou can add this project to your portfolio and discuss it during interviews to demonstrate your expertise in data engineering.",
      "skills_focused": "Databricks, Pig",
      "job_title": "Data Engineer"
    },
    {
      "date": "2026-01-29",
      "challenge": "**Challenge Title:** Automating Test Cases with CI/CD Pipeline\n\n**What You'll Build/Create:** A simple automated testing pipeline using GitHub Actions, Python, and Pytest, integrating with a CI/CD workflow.\n\n**Skills Practiced:**\n\n* Automation: Writing automated test cases using Pytest\n* CI/CD: Creating a GitHub Actions workflow to run automated tests\n* Agile/Scrum: Demonstrating a practical application of continuous integration and delivery\n\n**Step-by-Step Instructions:**\n\n1. Create a new GitHub repository and initialize it with a Python project using `python -m venv env` and `pip install pytest`.\n2. Write two simple test cases using Pytest in a file named `test_example.py`.\n3. Create a new GitHub Actions workflow file in `.github/workflows/ci-cd.yml`.\n4. Define a workflow that installs dependencies, runs the Pytest tests, and reports the results.\n5. Commit and push the changes to your GitHub repository.\n6. Verify that the GitHub Actions workflow runs successfully and displays the test results.\n\n**Success Criteria:**\n\n* The GitHub Actions workflow runs automatically on push events.\n* The Pytest tests pass, and the results are displayed in the workflow output.\n* The workflow is properly configured to install dependencies and run the tests.\n\n**How This Helps Your QA Engineer Job Search:**\n\nCompleting this challenge demonstrates your ability to automate test cases, integrate with CI/CD pipelines, and apply Agile/Scrum principles in a practical setting. This hands-on experience is portfolio-worthy and showcases your skills to potential employers. By including this project in your portfolio or discussing it during interviews, you can highlight your proficiency in automation, CI/CD, and Agile/Scrum, making you a more competitive candidate for QA Engineer roles.",
      "skills_focused": "Automation, CI/CD, Agile/Scrum",
      "job_title": "QA Engineer"
    },
    {
      "date": "2026-01-29",
      "challenge": "**Challenge Title:** Automating a Simple Web Application Test with GitHub Actions\n\n**What You'll Build/Create:** A basic automated test suite for a web application using Selenium WebDriver and GitHub Actions for Continuous Integration/Continuous Deployment (CI/CD).\n\n**Skills Practiced:**\n- Automation: Using Selenium WebDriver to automate web application testing\n- CI/CD: Integrating automated tests with GitHub Actions for continuous testing\n\n**Step-by-Step Instructions:**\n1. **Setup Environment**: Ensure you have Python installed on your machine. Install Selenium WebDriver using pip: `pip install selenium`.\n2. **Create a GitHub Repository**: Create a new repository on GitHub for your project.\n3. **Write Automated Tests**: Write a simple automated test using Selenium WebDriver that opens a webpage (e.g., https://www.google.com) and verifies the title of the page.\n4. **Configure GitHub Actions**: Create a new file in your repository under `.github/workflows/` named `automated_test.yml`. Configure this file to run your automated test on every push to the repository.\n5. **Push Changes to GitHub**: Push your test and GitHub Actions configuration to your repository.\n\n**Example Automated Test Code (Python):**\n```python\nfrom selenium import webdriver\n\ndef test_google_title():\n    driver = webdriver.Chrome()\n    driver.get(\"https://www.google.com\")\n    assert driver.title == \"Google\"\n    driver.quit()\n```\n\n**Example GitHub Actions Configuration (automated_test.yml):**\n```yaml\nname: Automated Test\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n      - name: Setup ChromeDriver\n        uses: nanasess/setup-chromedriver@v1\n      - name: Run Automated Test\n        run: |\n          python -m pytest test_google_title.py\n```\n\n**Success Criteria:**\n- Your automated test runs successfully on GitHub Actions when you push changes to your repository.\n- The test opens the specified webpage and correctly verifies its title.\n\n**How This Helps Your QA Engineer Job Search:**\nThis challenge demonstrates your ability to automate tests and integrate them into a CI/CD pipeline, which is a highly valued skill in the industry. By completing this challenge, you'll have a portfolio-worthy project that showcases your understanding of automation and CI/CD, making you a more competitive candidate for QA Engineer positions.",
      "skills_focused": "Automation, CI/CD",
      "job_title": "QA Engineer"
    },
    {
      "date": "2026-01-29",
      "challenge": "**Challenge Title:** Automating a Simple Web Application Test using Selenium and GitHub Actions\n\n**What You'll Build/Create:** A basic automated test suite for a simple web application using Selenium WebDriver, and integrate it with GitHub Actions for Continuous Integration/Continuous Deployment (CI/CD).\n\n**Skills Practiced:**\n\n* Automation testing using Selenium WebDriver\n* CI/CD pipeline setup using GitHub Actions\n* Basic understanding of automation frameworks and CI/CD tools\n\n**Step-by-Step Instructions:**\n\n1. **Setup**: Create a new GitHub repository and initialize it with a `README.md` file. Install Selenium WebDriver and the required bindings (e.g., Python or Java) on your local machine.\n2. **Create a Simple Web Application**: Use a simple web application like a calculator or a to-do list (e.g., https://todomvc.com/examples/react/#/) as the application under test.\n3. **Write Automated Tests**: Write 2-3 automated tests using Selenium WebDriver to cover basic functionality (e.g., adding a todo item, completing a todo item, and verifying the todo list).\n4. **Create a GitHub Actions Workflow**: Create a new YAML file in the `.github/workflows` directory to define a GitHub Actions workflow. This workflow should run your automated tests on every push to the repository.\n5. **Configure the Workflow**: Configure the workflow to use the Selenium WebDriver and run your automated tests. You can use a pre-built Selenium WebDriver image or install it as part of the workflow.\n6. **Commit and Push**: Commit your changes and push them to the GitHub repository.\n\n**Success Criteria:**\n\n* Your automated tests run successfully on every push to the repository.\n* The GitHub Actions workflow is triggered correctly and runs your tests.\n* You can see the test results and any failures in the GitHub Actions workflow logs.\n\n**How This Helps Your QA Engineer Job Search:**\n\n* Demonstrates your ability to automate tests using Selenium WebDriver.\n* Shows your understanding of CI/CD pipelines and GitHub Actions.\n* Provides a portfolio-worthy example of your automation skills.\n* Gives you hands-on experience with popular tools and technologies used in the industry.\n* Helps you develop a basic understanding of automation frameworks and CI/CD tools, making you a more competitive candidate for QA Engineer roles.",
      "skills_focused": "Automation, CI/CD",
      "job_title": "QA Engineer"
    },
    {
      "date": "2026-01-29",
      "challenge": "**Challenge Title:** Automating a Simple Web Application Test using Selenium and GitHub Actions\n\n**What You'll Build/Create:** A automated test suite for a simple web application using Selenium WebDriver, and integrate it with GitHub Actions for Continuous Integration/Continuous Deployment (CI/CD).\n\n**Skills Practiced:**\n\n* Automation: Using Selenium WebDriver to automate web application testing\n* Agile/Scrum: Understanding the importance of automation in Agile development\n* CI/CD: Integrating automated tests with GitHub Actions for continuous testing and deployment\n\n**Step-by-Step Instructions:**\n\n1. Create a new GitHub repository and initialize it with a `README.md` file.\n2. Install Selenium WebDriver and the necessary bindings (e.g., Python) on your local machine.\n3. Create a simple test script using Selenium WebDriver to automate a web application test (e.g., logging into a demo website).\n4. Create a new GitHub Actions workflow file (`.yml`) in the repository's `.github/workflows` directory.\n5. Configure the workflow to run the automated test script on every push to the repository.\n6. Push the changes to the repository and verify that the automated test runs successfully.\n\n**Success Criteria:**\n\n* The automated test script runs successfully and reports the test results.\n* The GitHub Actions workflow is triggered on every push to the repository.\n* The test results are visible in the GitHub Actions workflow run logs.\n\n**How This Helps Your QA Engineer Job Search:**\n\nCompleting this challenge demonstrates your ability to automate web application testing using Selenium WebDriver and integrate it with CI/CD tools like GitHub Actions. This is a valuable skill for QA Engineers, and having a portfolio-worthy example of this work can help you stand out in the job market. By showcasing your ability to automate testing and integrate with CI/CD pipelines, you can demonstrate your understanding of Agile/Scrum principles and your ability to contribute to a team's testing efforts.",
      "skills_focused": "Automation, Agile/Scrum, CI/CD",
      "job_title": "QA Engineer"
    },
    {
      "date": "2026-01-29",
      "challenge": "**Challenge Title:** \"Data Processing with Databricks and Pig\"\n\n**What You'll Build/Create:** A simple data processing pipeline using Databricks and Apache Pig to extract insights from a sample dataset.\n\n**Skills Practiced:**\n\n* Using Databricks to create and manage clusters\n* Writing Apache Pig scripts to process data\n* Loading and storing data in a Databricks database\n\n**Step-by-Step Instructions:**\n\n1. Create a free Databricks account and set up a new cluster with the latest Apache Spark version.\n2. Download the sample dataset (e.g., a CSV file containing information about employees) from a publicly available source (e.g., Kaggle).\n3. Upload the dataset to Databricks using the \"DBFS\" (Databricks File System) interface.\n4. Write an Apache Pig script to load the data, filter out rows with missing values, and group the data by department.\n5. Use the Pig script to store the processed data in a new table in the Databricks database.\n6. Verify the results by querying the new table using Spark SQL.\n\n**Success Criteria:**\n\n* The cluster is created and running successfully.\n* The Pig script executes without errors and produces the expected output.\n* The processed data is stored correctly in the new table.\n\n**How This Helps Your Data Engineer Job Search:**\n\nCompleting this challenge demonstrates your ability to work with Databricks and Apache Pig to process and analyze data. This is a valuable skill for Data Engineers, as it shows you can design and implement data processing pipelines using popular big data tools. By including this project in your portfolio, you can showcase your hands-on experience with Databricks and Pig, making you a more attractive candidate for Data Engineer roles.",
      "skills_focused": "Databricks, Pig",
      "job_title": "Data Engineer"
    },
    {
      "date": "2026-01-29",
      "challenge": "**Challenge Title:** Building a Simple Data Pipeline with Databricks and Azure\n\n**What You'll Build/Create:** A basic data pipeline that ingests data from Azure Blob Storage, processes it using Databricks, and writes the output to Azure SQL Database.\n\n**Skills Practiced:**\n\n* Setting up a Databricks workspace in Azure\n* Creating a cluster and configuring it for data processing\n* Ingesting data from Azure Blob Storage using Databricks\n* Processing data using Spark SQL and Databricks notebooks\n* Writing data to Azure SQL Database\n\n**Step-by-Step Instructions:**\n\n1. **Create an Azure account**: If you haven't already, sign up for a free Azure account.\n2. **Create a Databricks workspace**: Navigate to the Azure portal and create a new Databricks workspace.\n3. **Create a cluster**: Create a new cluster in your Databricks workspace with the default settings.\n4. **Create an Azure Blob Storage container**: Create a new container in Azure Blob Storage and upload a sample CSV file (e.g., a list of employees).\n5. **Create a Databricks notebook**: Create a new notebook in your Databricks workspace and use Spark SQL to read the CSV file from Azure Blob Storage.\n6. **Process the data**: Use Spark SQL to process the data (e.g., filter, aggregate, or transform the data).\n7. **Write data to Azure SQL Database**: Create a new Azure SQL Database and write the processed data to a table using Spark SQL.\n8. **Verify the results**: Verify that the data has been successfully written to the Azure SQL Database.\n\n**Success Criteria:**\n\n* Data is successfully ingested from Azure Blob Storage into Databricks.\n* Data is processed correctly using Spark SQL in the Databricks notebook.\n* Processed data is written to Azure SQL Database.\n\n**How This Helps Your Data Engineer Job Search:**\n\nCompleting this challenge demonstrates your ability to:\n\n* Set up and configure a Databricks workspace in Azure\n* Ingest and process data using Spark SQL and Databricks notebooks\n* Integrate with other Azure services (e.g., Azure Blob Storage and Azure SQL Database)\n\nThis challenge showcases your skills in building a basic data pipeline, which is a fundamental task for Data Engineers. By including this project in your portfolio, you can demonstrate your hands-on experience with Databricks and Azure to potential employers.",
      "skills_focused": "Databricks, Azure",
      "job_title": "Data Engineer"
    }
  ],
  "skills_learned": []
}